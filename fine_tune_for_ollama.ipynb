{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ca04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Python: /home/manuelbomi/fine_tune_LLM\n",
      " Torch: 2.9.1+cu128\n",
      " CUDA: True\n",
      "  GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      " GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "#### Environment check & imports\n",
    "# OVERVIEW: This cell sets up the Python environment by importing essential libraries, checking GPU availability for accelerated training, \n",
    "# and clearing any leftover GPU memory from previous sessions.\n",
    "\n",
    "#### Environment check & imports\n",
    "# Cell 1: Environment setup\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch        # PyTorch - deep learning framework\n",
    "import json         # JSON handling for reading/writing data files\n",
    "import os           # Operating system interfaces for file paths\n",
    "\n",
    "# Print current working directory - shows where the script is running from\n",
    "print(f\" Python: {os.getcwd()}\")\n",
    "\n",
    "# Display PyTorch version - important for compatibility\n",
    "print(f\" Torch: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available - crucial for training speed\n",
    "print(f\" CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# If CUDA is available, get the GPU name; otherwise show 'None'\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Clear GPU memory cache from any previous PyTorch operations\n",
    "# This prevents \"out of memory\" errors from residual memory\n",
    "torch.cuda.empty_cache()\n",
    "print(f\" GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e2747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 500 examples\n",
      "Sample: {'input': 'Extract subscription usage details:\\nCustomer 10001 used Apple Music on Game Console under the Basic plan costing $16.23 in region AU.', 'output': {'customer_id': 'CUST-10001', 'service_name': 'Apple Music', 'subscription_plan': 'Basic', 'monthly_price_usd': 16.23, 'device_type': 'Game Console', 'region': 'AU', 'usage_hours': 3.97, 'event_timestamp': '2024-12-02T00:00:00'}}\n"
     ]
    }
   ],
   "source": [
    "## Load data \n",
    "# OVERVIEW: This cell loads the training data from a JSON file, displaying how many examples are available and showing a sample to verify data format.\n",
    "# Open and read the JSON file containing training data\n",
    "# 'with open' ensures proper file closure even if errors occur\n",
    "with open(\"customer_subscription_traceability.json\", \"r\") as f:\n",
    "    # Load JSON content into a Python data structure (list/dictionary)\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the number of training examples loaded\n",
    "print(f\" Loaded {len(data)} examples\")\n",
    "\n",
    "# Display the second example (index 1) to show data structure\n",
    "# Shows what kind of data we're working with\n",
    "print(f\"Sample: {data[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe17f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Unsloth models for 4-bit fine-tuning:\n",
      " 1. unsloth/tinyllama-bnb-4bit\n",
      " 2. unsloth/llama-3.2-3b-bnb-4bit\n",
      " 3. unsloth/Phi-3-mini-4k-instruct-bnb-4bit\n",
      " 4. unsloth/phi-2\n",
      " 5. unsloth/gemma-2b-bnb-4bit\n",
      " 6. unsloth/Qwen2.5-1.5B-bnb-4bit\n",
      "\n",
      " Selected: unsloth/tinyllama-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "### List available models & Model Selection\n",
    "\n",
    "# OVERVIEW: This cell presents a menu of available models optimized for 4-bit quantization, allowing selection of which model to \n",
    "# fine-tune based on size and performance needs.\n",
    "# Define a list of available models that are compatible with Unsloth's 4-bit quantization\n",
    "# 4-bit quantization reduces memory usage by 75% while maintaining good performance\n",
    "available_models = [\n",
    "    \"unsloth/tinyllama-bnb-4bit\",           # 1.1B parameters - Smallest, fastest\n",
    "    \"unsloth/llama-3.2-3b-bnb-4bit\",        # 3B parameters - Latest Llama version\n",
    "    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",  # 3.8B parameters - Phi-3 model\n",
    "    \"unsloth/phi-2\",                         # 2.7B parameters - Older Phi model\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",            # 2B parameters - Google's model\n",
    "    \"unsloth/Qwen2.5-1.5B-bnb-4bit\",        # 1.5B parameters - Good for multilingual\n",
    "]\n",
    "\n",
    "print(\"Available Unsloth models for 4-bit fine-tuning:\")\n",
    "# Enumerate models starting from 1 for user-friendly display\n",
    "for i, model in enumerate(available_models, 1):\n",
    "    print(f\"{i:2}. {model}\")\n",
    "\n",
    "# Select which model to use - this is a configuration variable\n",
    "# Change this to experiment with different models\n",
    "MODEL_CHOICE = \"tinyllama\"  # Change this to try different models\n",
    "\n",
    "# Create a mapping from short names to full model identifiers\n",
    "# Makes it easier to switch between models\n",
    "model_map = {\n",
    "    \"tinyllama\": \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"llama3.2-3b\": \"unsloth/llama-3.2-3b-bnb-4bit\",\n",
    "    \"phi-2\": \"unsloth/phi-2\",\n",
    "    \"gemma-2b\": \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"qwen1.5b\": \"unsloth/Qwen2.5-1.5B-bnb-4bit\",\n",
    "}\n",
    "\n",
    "# Get the actual model name based on the user's choice\n",
    "model_name = model_map[MODEL_CHOICE]\n",
    "print(f\"\\n Selected: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a55f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading unsloth/tinyllama-bnb-4bit with max_seq_length=1024...\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      " Loaded unsloth/tinyllama-bnb-4bit\n",
      "   Parameters: 615,606,272\n"
     ]
    }
   ],
   "source": [
    "### Load model and tokenizer\n",
    "\n",
    "## OVERVIEW: This cell loads the selected model and tokenizer with memory-efficient 4-bit quantization and configures\n",
    "#  the tokenizer for proper batching.\n",
    "\n",
    "# Import Unsloth's FastLanguageModel - optimized for faster training\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Define maximum sequence length for the model\n",
    "# Shorter = faster training, longer = more context\n",
    "max_seq_length = 1024\n",
    "print(f\"Loading {model_name} with max_seq_length={max_seq_length}...\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer with optimized settings\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,      # Which model to load\n",
    "    max_seq_length=max_seq_length,  # Maximum input length\n",
    "    dtype=None,                 # Auto-detect best data type\n",
    "    load_in_4bit=True,          # Load in 4-bit quantization (reduces memory)\n",
    ")\n",
    "\n",
    "# Fix tokenizer issues - some tokenizers don't have a pad token defined\n",
    "# Pad token is needed for batching inputs of different lengths\n",
    "if tokenizer.pad_token is None:\n",
    "    # Use end-of-sequence token as pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\" Loaded {model_name}\")\n",
    "# Calculate and display total number of parameters in the model\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preparing training data...\n",
      "   Dataset created: 500 examples\n",
      "\n",
      " Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2026.1.3 patched 22 layers with 22 QKV layers, 22 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LoRA added: 9,011,200 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "## Prepare data and LoRA\n",
    "\n",
    "# OVERVIEW: This cell formats the training data into the proper chat template structure and applies LoRA (parameter-efficient fine-tuning) \n",
    "# to the model, drastically reducing the number of trainable parameters.\n",
    "\n",
    "## Prepare data and LoRA\n",
    "\n",
    "# Import Dataset class from Hugging Face datasets library\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"\\n Preparing training data...\")\n",
    "\n",
    "def format_for_training(example):\n",
    "    \"\"\"Format training examples in TinyLlama chat format\"\"\"\n",
    "    # Extract input text from example\n",
    "    input_text = example['input']\n",
    "    # Convert output to JSON string\n",
    "    output_json = json.dumps(example['output'])\n",
    "    # Format in TinyLlama chat template with special tokens\n",
    "    return f\"<|system|>Extract JSON data from subscription information.<|end|>\\n<|user|>\\n{input_text}<|end|>\\n<|assistant|>\\n{output_json}<|end|>\"\n",
    "\n",
    "# Apply formatting function to all training examples\n",
    "formatted_data = [format_for_training(item) for item in data]\n",
    "\n",
    "# Convert list to Hugging Face Dataset object (optimized for training)\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"   Dataset created: {len(dataset)} examples\")\n",
    "\n",
    "# Add LoRA (Low-Rank Adaptation) to the model\n",
    "print(\"\\n Adding LoRA adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,                # Base model to adapt\n",
    "    r=32,                # LoRA rank - higher = more capacity but more parameters\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n",
    "    lora_alpha=64,       # Scaling factor for LoRA weights\n",
    "    lora_dropout=0,      # No dropout for LoRA layers\n",
    "    bias=\"none\",         # Don't train bias parameters\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Save memory during training\n",
    "    random_state=3407,   # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Calculate how many parameters will actually be trained (LoRA adapters only)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\" LoRA added: {trainable_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88663077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Setting up training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e19b5e6de9441c694f3565967a5ecb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting training for unsloth/tinyllama-bnb-4bit...\n",
      "   Batch: 4 Ã— 4 = 16\n",
      "   Estimated time: 10-30 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 3 | Total steps = 96\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 9,011,200 of 1,109,059,584 (0.81% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model saved to: tinyllama-finetuned/\n"
     ]
    }
   ],
   "source": [
    "##  Training pipeline\n",
    "#  OVERVIEW: This cell configures and executes the training process with optimized settings for memory efficiency and performance, \n",
    "# then saves the fine-tuned model.\n",
    "\n",
    "# Import training components\n",
    "from trl import SFTTrainer           # Supervised Fine-Tuning trainer\n",
    "from transformers import TrainingArguments  # Training configuration\n",
    "\n",
    "print(\"\\n  Setting up training...\")\n",
    "\n",
    "# Batch size configuration\n",
    "batch_size = 4                       # Examples per GPU\n",
    "gradient_accumulation = 4            # Accumulate gradients over multiple steps\n",
    "effective_batch = batch_size * gradient_accumulation  # True batch size\n",
    "\n",
    "# Configure training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"outputs-{MODEL_CHOICE}\",  # Where to save checkpoints\n",
    "    num_train_epochs=3,                    # Number of full training passes\n",
    "    per_device_train_batch_size=batch_size,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=gradient_accumulation,  # Simulate larger batch\n",
    "    warmup_steps=10,                     # Linear warmup at start\n",
    "    learning_rate=2e-4,                  # How fast to learn\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use mixed precision training\n",
    "    bf16=torch.cuda.is_bf16_supported(), # Use bfloat16 if GPU supports it\n",
    "    logging_steps=10,                    # Log progress every 10 steps\n",
    "    optim=\"adamw_8bit\",                  # Memory-efficient optimizer\n",
    "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
    "    lr_scheduler_type=\"cosine\",          # Learning rate schedule\n",
    "    seed=3407,                           # Random seed for reproducibility\n",
    "    save_strategy=\"epoch\",               # Save after each epoch\n",
    "    save_total_limit=2,                  # Keep only 2 checkpoints\n",
    "    report_to=\"none\",                    # Don't report to external services\n",
    "    remove_unused_columns=False,         # Keep all columns for debugging\n",
    "    dataloader_pin_memory=False,         # Don't pin memory (saves RAM)\n",
    ")\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                    # The model to train\n",
    "    tokenizer=tokenizer,            # Tokenizer for text processing\n",
    "    train_dataset=dataset,          # Training data\n",
    "    dataset_text_field=\"text\",      # Field containing text in dataset\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "    args=training_args,             # Training configuration\n",
    ")\n",
    "\n",
    "print(f\" Starting training for {model_name}...\")\n",
    "print(f\"   Batch: {batch_size} Ã— {gradient_accumulation} = {effective_batch}\")\n",
    "print(f\"   Estimated time: 10-30 minutes\")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "output_dir = f\"{MODEL_CHOICE}-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"\\n Model saved to: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test response:\n",
      "<|system|>Extract JSON data from subscription information.<|end|>\n",
      "<|user|>\n",
      "Extract subscription usage details:\n",
      "Customer 10001 used Apple Music on Game Console under the Basic plan costing $16.23 in region AU.<|end|>\n",
      "<|assistant|>\n",
      "{\"customer_id\": \"CUST-10001\", \"service_name\": \"Apple Music\", \"subscription_plan\": \"Basic\", \"monthly_price_usd\": 16.23, \"device_type\": \"Game Console\", \"region\": \"AU\", \"usage_hours\": 4.7, \"event_timestamp\": \"2024-01-01T00:00:0\n"
     ]
    }
   ],
   "source": [
    "# Test with direct tokenization (since TinyLlama doesn't have chat_template)\n",
    "## OVERVIEW: This cell performs a basic test of the fine-tuned model by formatting a prompt, generating a response, and displaying the output.\n",
    "\n",
    "# Test with direct tokenization (since TinyLlama doesn't have chat_template)\n",
    "# Enable inference mode (disables dropout, uses less memory)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Create a test prompt similar to training examples\n",
    "test_prompt = \"Extract subscription usage details:\\nCustomer 10001 used Apple Music on Game Console under the Basic plan costing $16.23 in region AU.\"\n",
    "\n",
    "# Manually format the prompt using the same template as training\n",
    "formatted_prompt = f\"<|system|>Extract JSON data from subscription information.<|end|>\\n<|user|>\\n{test_prompt}<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "# Tokenize the input (convert text to numbers)\n",
    "inputs = tokenizer(\n",
    "    [formatted_prompt],      # Wrap in list for batch processing\n",
    "    return_tensors=\"pt\",     # Return PyTorch tensors\n",
    "    padding=True,            # Pad to same length if multiple inputs\n",
    ").to(\"cuda\")                 # Move to GPU for faster processing\n",
    "\n",
    "# Generate a response from the model\n",
    "outputs = model.generate(\n",
    "    **inputs,                # Pass tokenized inputs\n",
    "    max_new_tokens=100,      # Maximum tokens to generate\n",
    "    temperature=0.3,         # Lower = more deterministic, higher = more creative\n",
    "    do_sample=True,          # Use sampling instead of greedy decoding\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Token to use for padding\n",
    ")\n",
    "\n",
    "# Convert generated token IDs back to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\n Test response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25950361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing fine-tuned TinyLlama...\n",
      "\n",
      "============================================================\n",
      "Test 1: Extract subscription usage details:\n",
      "Customer 10001 used Appl...\n",
      "Response: {\"customer_id\": \"CUST-10001\", \"service_name\": \"Apple Music\", \"subscription_plan\": \"Basic\", \"\n",
      " Not JSON format\n",
      "\n",
      "============================================================\n",
      "Test 2: Extract subscription usage details:\n",
      "Customer 10005 used Netf...\n",
      "Response: {\"customer_id\": \"CUST-10005\", \"service_name\": \"Netflix\", \"subscription_plan\":\n",
      " Not JSON format\n",
      "\n",
      "============================================================\n",
      "Test 3: Extract subscription usage details:\n",
      "Customer 10100 used Disn...\n",
      "Response: {\"customer_id\": \"CUST-10100\", \"service_name\": \"Disney+\", \"subscription_plan\": \"Standard\", \"\n",
      " Not JSON format\n"
     ]
    }
   ],
   "source": [
    "##  It Could be Better to Create a proper test function\n",
    "## OVERVIEW: This cell creates a robust testing function that properly formats prompts, generates responses with settings optimized for JSON output, \n",
    "# and validates whether the model produces valid JSON.\n",
    "\n",
    "\n",
    "def test_model(model, tokenizer, test_input):\n",
    "    \"\"\"Test the fine-tuned model with proper formatting\"\"\"\n",
    "    # Format exactly like training data for consistent behavior\n",
    "    formatted_prompt = f\"<|system|>Extract JSON data from subscription information.<|end|>\\n<|user|>\\n{test_input}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        [formatted_prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response with settings optimized for JSON output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,      # Allow more tokens for JSON\n",
    "        temperature=0.1,         # Very low temperature for consistent JSON structure\n",
    "        do_sample=False,         # Greedy decoding for predictable output\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode the full generated text\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response (after the assistant token)\n",
    "    if \"<|assistant|>\" in full_response:\n",
    "        # Split at assistant token and take the last part\n",
    "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "        return response\n",
    "    else:\n",
    "        # If formatting wasn't preserved, return full response\n",
    "        return full_response\n",
    "\n",
    "# Test multiple examples to evaluate model performance\n",
    "print(\"\\n Testing fine-tuned TinyLlama...\")\n",
    "\n",
    "# Define different test cases to check generalization\n",
    "test_cases = [\n",
    "    \"Extract subscription usage details:\\nCustomer 10001 used Apple Music on Game Console under the Basic plan costing $16.23 in region AU.\",\n",
    "    \"Extract subscription usage details:\\nCustomer 10005 used Netflix on Smart TV under Premium plan costing $15.99 in US region\",\n",
    "    \"Extract subscription usage details:\\nCustomer 10100 used Disney+ on Tablet under Standard plan costing $12.50 in CA region\",\n",
    "]\n",
    "\n",
    "# Test each case and evaluate the results\n",
    "for i, test_prompt in enumerate(test_cases, 1):\n",
    "    # Print separator for readability\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    # Show truncated prompt for context\n",
    "    print(f\"Test {i}: {test_prompt[:60]}...\")\n",
    "    \n",
    "    # Get model response\n",
    "    response = test_model(model, tokenizer, test_prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Check if output is valid JSON format\n",
    "    if response.strip().startswith(\"{\") and response.strip().endswith(\"}\"):\n",
    "        print(\" Valid JSON output!\")\n",
    "    else:\n",
    "        print(\" Not JSON format\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
